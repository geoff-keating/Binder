{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas scikit-learn numpy matplotlib torch seaborn\n",
    "!kaggle competitions download titanic\n",
    "!unzip -o titanic.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model I: Birkenhead Model\n",
    "\n",
    "To get things going, we can make an easy first pass model that operates on the knowledge that women and children were prioritized evacuation under the [Birkenhead drill](https://en.wikipedia.org/wiki/Women_and_children_first).  Simply put, if you are a woman or under the age of 13, the model will predict that the individual survives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_age = 12.0\n",
    "\n",
    "# Keep sex and age columns\n",
    "reduced = train[[\"Sex\", \"Age\", \"Survived\"]].copy()\n",
    "reduced.Sex = (reduced.Sex == \"female\").astype(int)\n",
    "reduced.Age = (reduced.Age <= child_age).astype(int)\n",
    "reduced[\"predicted\"] = reduced.Sex | reduced.Age\n",
    "train_acc = (reduced.predicted == reduced.Survived).mean()\n",
    "sex_only = (reduced.Sex == reduced.Survived).mean()\n",
    "print(f\"Birkenhead drill train accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"Sex only train accuracy: {sex_only * 100:.2f}%\")\n",
    "\n",
    "# Write out the submission\n",
    "bh_test = test[[\"PassengerId\", \"Sex\", \"Age\"]].copy()\n",
    "bh_test.Sex = (bh_test.Sex == \"female\").astype(int)\n",
    "bh_test.Age = (bh_test.Age <= child_age).astype(int)\n",
    "bh_test[\"Survived\"] = bh_test.Sex | bh_test.Age\n",
    "bh_test.to_csv(\"birkenhead.csv\", columns=[\"PassengerId\", \"Survived\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit titanic -f birkenhead.csv -m \"Naive Model\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  For such a simple model we are nearly 80% accurate.  Adjusting for children didn't make much of a difference, but it did give us an extra half a percent or so.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model II: Logistic Regression with Simple Imputation\n",
    "\n",
    "Let's get all datapoints involved by using a simple linear model to classify survival.  We still have to deal with the case where data is missing from some columns, so let's apply the simplest imputation strategy we can derive: modal imputation.  We'll just fill all missing values with the most common value for that column and move on.  This will help establish a baseline for future approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What columns lack entries?\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_data_prep(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned = data.copy(deep=True)\n",
    "    cleaned.Sex = (cleaned.Sex == \"female\").astype(int)\n",
    "\n",
    "    # Drop complex string fields like name, Cabin, and Ticket\n",
    "    cleaned.drop(columns=[\"Name\", \"Cabin\", \"Ticket\"], inplace=True)\n",
    "\n",
    "    # Impute Age, Embarked\n",
    "    cleaned.Age.fillna(cleaned.Age.median(), inplace=True)\n",
    "    cleaned.Fare.fillna(cleaned.Fare.median(), inplace=True)\n",
    "    cleaned.Embarked.fillna(cleaned.Embarked.mode().iloc[0], inplace=True)\n",
    "\n",
    "    # Get dummy variables for Embarked\n",
    "    cleaned = pd.get_dummies(cleaned, columns=[\"Embarked\"])\n",
    "\n",
    "    # Log fare to get rid of long tails\n",
    "    cleaned.Fare = np.log10(cleaned.Fare + 1)\n",
    "\n",
    "    # Normalize age\n",
    "    cleaned.Age = cleaned.Age / cleaned.Age.max()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = simple_data_prep(train)\n",
    "test_clean = simple_data_prep(test)\n",
    "train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using logistic regression\n",
    "lgr = LogisticRegression(max_iter=1000, verbose=True)\n",
    "x_cols = [x for x in train_clean.columns if x not in [\"Survived\", \"PassengerId\"]]\n",
    "print(x_cols)\n",
    "lgr.fit(train_clean[x_cols], y=train_clean.Survived)\n",
    "train_acc = np.mean(lgr.predict(train_clean[x_cols]) == train_clean[\"Survived\"]) * 100\n",
    "print(f\"Accuracy = {train_acc:.2f}\")\n",
    "\n",
    "test_clean[\"Survived\"] = lgr.predict(test_clean[x_cols])\n",
    "test_clean[[\"PassengerId\", \"Survived\"]].to_csv(\"simple_lgr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit titanic -f simple_lgr.csv -m \"Simple imputation and logistic regression\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sophisticated Imputation\n",
    "\n",
    "Before trying out any more sophisticated models, let's try to clean up our data to get the most out of what we have.\n",
    "\n",
    "## Breaking Down String Fields\n",
    "\n",
    "String fields like Cabin, Ticket, Name, and Embarked cannot be inputs to most models at this point and must be parsed into numerical fields.  Fortunately, these fields have lots of rich structure that embed information about the passenger.  This section will deal with breaking information out of these fields into subfields that we can use to impute missing fields and ultimately train a model on.\n",
    "\n",
    "### Passenger Names\n",
    "\n",
    "Fortunately, passenger's names on the ticket reveal quite a bit of information about their status, which is highly correlated to their age.  For example:\n",
    "\n",
    "- \"Master\" is a honorific for young men or boys, meaning the age is likely under 18 years.  \n",
    "- Married women are given \"Mrs\" while unmarried women or girls are \"Miss\"\n",
    "- Some married women have their husbands name in parentheses. Some do not, possibly implying a widow?  Widows tend to be older \n",
    "\n",
    "These features in the name field can be extracted to binary variables which can be used to impute ages.\n",
    "\n",
    "### Tickets\n",
    "\n",
    "Some tickets are simply numbered, but others have letters or other encodings beforehand.  To extract meaningful information from the ticket field, split out any letters, or what we will call \"ticket modifiers\" and numbers, or \"ticket numbers\".  Modifiers will be mapped to an integer encoding with 0 being no characters in front of a ticket number.  Tickets with only letters will have ticket number 0.\n",
    "\n",
    "### Cabin Number\n",
    "\n",
    "The least populated field, cabin number seems to encode a number of things - the deck of the ship, the room number, and possibly the number of cabins this passenger has purchased.  Some cabin entries only have a deck associated with them and some have multiple entries.  We can break this field down into three variables: \n",
    "- An integer encoding for the deck the passenger was assigned to\n",
    "- The cabin number or first cabin number in the first cabin entry\n",
    "- The number of cabin numbers or spaces in the cabin entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which fields have missing entries in both datasets\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_names(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process passenger names into several columns that represent title information in a categorical\n",
    "    variable\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A dataframe with column \"Name\"\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with new columns derived from \"Name\"\n",
    "    \"\"\"\n",
    "    modified = data.copy(deep=True)\n",
    "\n",
    "    # Develop columns based on the passenger's name\n",
    "    modified[\"master\"] = (data.Name.str.contains(\"master\", case=False)).astype(int)\n",
    "    modified[\"mrs\"] = (data.Name.str.contains(\"Mrs\", case=True)).astype(int)\n",
    "    modified[\"miss\"] = (data.Name.str.contains(\"Miss\", case=True)).astype(int)\n",
    "    modified[\"widow\"] = (modified[\"mrs\"] & ~data.Name.str.contains(\"\\(\")).astype(int)\n",
    "    modified[\"mr\"] = (data.Name.str.contains(\"Mr.\", case=True)).astype(int)\n",
    "    modified.drop(columns=[\"Name\"], inplace=True)\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticket_to_number(ticket):\n",
    "    if isinstance(ticket, (int, float)):\n",
    "        return int(ticket)\n",
    "    else:\n",
    "        no_letters = \"\".join(filter(str.isdigit, ticket))\n",
    "        if len(no_letters):\n",
    "            return int(no_letters)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "def process_ticket(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    modified = data.copy(deep=True)\n",
    "    modified[\"ticket_modifier\"] = modified.Ticket.apply(lambda x: int(len(list(filter(str.isalpha, x))) > 0))\n",
    "    modified[\"ticket_number\"] = modified.Ticket.apply(ticket_to_number)\n",
    "    modified.drop(columns=[\"Ticket\"], inplace=True)\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number(cabin_entry):\n",
    "    \"\"\"Extract a cabin number if one exists or take the mean of all numbers found\"\"\"\n",
    "    if isinstance(cabin_entry, (int, float)):\n",
    "        return int(cabin_entry)\n",
    "    else:\n",
    "        entries = cabin_entry.split(\" \")\n",
    "        numbers = [\"\".join(filter(str.isdigit, x)) for x in entries]\n",
    "        numbers = [int(x) for x in numbers if len(x)]\n",
    "        if len(numbers) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return sum(numbers) / len(numbers)\n",
    "\n",
    "\n",
    "def process_cabin(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    modified = data.copy(deep=True)\n",
    "    # Develop columns based on the cabin\n",
    "    with_cabins = modified[~modified.Cabin.isna()]\n",
    "    modified.loc[~modified.Cabin.isna(), \"cabin_entries\"] = with_cabins.Cabin.apply(\n",
    "        lambda x: len(str(x).split(\" \"))\n",
    "    )\n",
    "\n",
    "    # Develop an encoding for the deck level, leave missing ones as NA\n",
    "    for idx, level in enumerate([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]):\n",
    "        modified.loc[\n",
    "            ~modified.Cabin.isna() & with_cabins.Cabin.str.contains(level, case=False),\n",
    "            \"cabin_level\",\n",
    "        ] = (\n",
    "            idx + 1\n",
    "        )\n",
    "    modified.drop(columns=[\"Cabin\"], inplace=True)\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string_fields(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    modified = data.copy(deep=True)\n",
    "\n",
    "    # Process Sex to be binary\n",
    "    modified.Sex = (modified.Sex == \"female\").astype(int)\n",
    "    # Get dummy variables for embarked\n",
    "    modified = pd.get_dummies(modified, columns=[\"Embarked\"])\n",
    "    modified = process_names(modified)\n",
    "    modified = process_cabin(modified)\n",
    "    modified = process_ticket(modified)\n",
    "    return modified"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted values from string fields, let's look at correlations between different variables prior to imputation.  This will show us which variables may be important in the broader goal of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputed = process_string_fields(train)\n",
    "train_imputed.drop(columns='PassengerId', inplace=True)\n",
    "corr = train_imputed.corr()\n",
    "sb.set(rc={'figure.figsize':(20,20)})\n",
    "sb.heatmap(corr, cmap=\"Blues\", annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are major correlations between survival and sex, class, fare, and extracted proxies for these characteristics such as title.  There appears to be very weak correlations between survival and cabin, embarked, and ticket variables, which isn't terribly surprising, but some of these variables do correlate strongly with age, so we can leverage them to impute age using nearest neighbors methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns= ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'master', 'mrs', 'miss', 'widow', 'mr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    modified = data.copy(deep=True)\n",
    "\n",
    "    # Process string fields\n",
    "    modified = process_string_fields(modified)\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    modified[modified.columns] = imputer.fit_transform(modified.values)\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    modified.drop(columns=[x for x in modified.columns if x not in desired_columns], inplace=True)\n",
    "\n",
    "    return modified"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training on this data, we have to normalize values as linear models do not work well with variables of different scale.  For large, long tailed distributions like Fare, we can take the log and normalize accordingly.  For all others, a simple rescaling will do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    modified = data.copy(deep=True)\n",
    "\n",
    "    # Normalize fare by log10\n",
    "    modified.Fare = np.log10(modified.Fare + 1)\n",
    "\n",
    "    # Normalize all columns\n",
    "    for column in modified.columns:\n",
    "        min_max = (modified[column].max() - modified[column].min())\n",
    "        if min_max != 0:\n",
    "            modified[column] = (modified[column] - modified[column].min()) / min_max\n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm = normalize(preprocess(train))\n",
    "test_norm = normalize(preprocess(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After formatting our training data, now we can fit a number of models to check what may be an optimal classifier.  In trials, the difference between a random forest, SVM, kernelized SVM, and logistic regression doesn't seem to be good.  Each shows some amount of overfitting, which may be a product of our imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = train_test_split(train_norm, test_size=.2)\n",
    "classifiers = [LogisticRegression(max_iter=500), SVC(), SVC(kernel='poly'), RandomForestClassifier()]\n",
    "train_x = train_split[[x for x in train_split.columns if x not in ['Survived', 'PassengerId']]]\n",
    "train_y = train_split['Survived']\n",
    "val_x = val_split[[x for x in val_split.columns if x not in ['Survived', 'PassengerId']]]\n",
    "val_y = val_split['Survived']\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print('Fitting ' + classifier.__class__.__name__)\n",
    "    classifier.fit(train_x, train_y)\n",
    "    train_acc = np.mean(classifier.predict(train_x) == train_y) * 100\n",
    "    val_acc = np.mean(classifier.predict(val_x) == val_y) * 100\n",
    "    print(train_acc)\n",
    "    print(val_acc)\n",
    "    test_frame = test.copy(deep=True)\n",
    "    test_frame['Survived'] = classifier.predict(test_norm)\n",
    "    test_frame['Survived'] = test_frame['Survived'].astype(int)\n",
    "    test_frame[['PassengerId', 'Survived']].to_csv(f'{classifier.__class__.__name__.lower()}_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions submit titanic -f <output> -m \"Age imputation with KNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c5e7bc60812e4f465d8a8f03ded68579c8c0c964eec928bebe0bf95b0baae9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
