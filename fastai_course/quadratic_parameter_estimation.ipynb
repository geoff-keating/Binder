{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Approaches to Quadratic Parameter Estimation\n",
    "\n",
    "This notebook is an extension of the FastAI course's \"How does a neural net really work?\" notebook that introduces users to `autograd` and `Tensor`.  To best get a feel for the lowest level of PyTorch after working with Lightning for some time, this is a quick trip back to the fundamentals.\n",
    "\n",
    "This notebook is split into the following sections:\n",
    "\n",
    "1. Define a few functions to build arbitrary quadratic functions and samplers that will add some noise to simulate noisy data derived from these functions\n",
    "2. Classic quadratic regression using the Moore-Penrose pseudoinverse to find the solution to the least squares problem.  This will serve as the gold standards for other methods\n",
    "3. Three parameter model using `torch` and attempt to find the quadratic parameters through gradient descent\n",
    "4. The same three parameter model using closed form solutions for the gradient of the loss function\n",
    "5. Using `scipy` to leverage some well designed optimizers to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_factory(a: float, b: float, c: float) -> callable:\n",
    "    \"\"\"Build a function that will return values defined by the quadratic function\n",
    "    parametrized by a, b, and c\n",
    "\n",
    "    Args:\n",
    "        a (float): The squared coefficient\n",
    "        b (float): The linear coefficient\n",
    "        c (float): The constant value\n",
    "\n",
    "    Returns:\n",
    "        callable: A function that returns 'y' values for the quadratic function\n",
    "            parametrized by a, b, and c\n",
    "    \"\"\"\n",
    "\n",
    "    def quadratic(x: float | np.ndarray) -> float | np.ndarray:\n",
    "        \"\"\"The quadratic function\"\"\"\n",
    "        return a * x**2 + b * x + c\n",
    "\n",
    "    return quadratic\n",
    "\n",
    "\n",
    "def noisy_samples(quadratic: callable, sigma: float) -> callable:\n",
    "    \"\"\"Wrap a defined quadratic function with a noise generator\n",
    "    to generate training samples\n",
    "\n",
    "    Args:\n",
    "        quadratic (callable): The defined quadratic function, see above\n",
    "        sigma (float): The variance of the gaussian noise added to every sample\n",
    "\n",
    "    Returns:\n",
    "        callable: A function that returns noisy samples of specified shape\n",
    "    \"\"\"\n",
    "\n",
    "    def noisy_quad(x: float | np.ndarray):\n",
    "        return quadratic(x) + np.random.randn(*x.shape) * sigma\n",
    "\n",
    "    return noisy_quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function and plot it\n",
    "a = 4\n",
    "b = 3\n",
    "c = -1\n",
    "quad_func = quadratic_factory(a, b, c)\n",
    "sampler = noisy_samples(quadratic=quad_func, sigma=5)\n",
    "\n",
    "x = np.linspace(-5, 5, 50)\n",
    "y = quad_func(x)\n",
    "\n",
    "# Generate some samples between [-5, 5]\n",
    "x_sampled = (np.random.rand(50) - 0.5) * 10\n",
    "y_sampled = sampler(x_sampled)\n",
    "\n",
    "plt.plot(x, y, label=\"F(x)\")\n",
    "plt.plot(x_sampled, y_sampled, \"r.\", label=\"Sampled F(x)\")\n",
    "plt.title(f\"f(x) = ${a}x^2 + {b}x + {c}$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Regression\n",
    "\n",
    "The problem we are trying to solve -- finding the optimal coefficients for an unknown quadratic function -- can be solved closed-form using simple regression.  This is a closed form solution for the following cost function:\n",
    "\n",
    "$$ \\lVert x \\lVert_2 = \\sqrt{\\sum_{i=0}^{N} (\\hat{y_i} - y_i)^2} $$\n",
    "\n",
    "However, inverting matrices takes time and more memory than a 3 parameter gradient descent.  Let's draw a baseline with regression and see how it stacks up to gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_regression(x: np.ndarray, y: np.ndarray) -> dict:\n",
    "    \"\"\"Run regression on noisy data to minimize the mean squared error\n",
    "    over three parameters\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): The input x coordinates\n",
    "        y (np.ndarray): The input y coordinates\n",
    "\n",
    "    Returns:\n",
    "        dict: information summarizing the regression\n",
    "    \"\"\"\n",
    "    matrix_x = np.vstack((x**2, x, np.ones_like(x))).T\n",
    "    matrix_x_inv = np.dot(np.linalg.inv(np.dot(matrix_x.T, matrix_x)), matrix_x.T)\n",
    "    best_fit = np.dot(matrix_x_inv, y)\n",
    "    y_predict = best_fit[0] * x**2 + best_fit[1] * x + best_fit[2]\n",
    "    cost = np.mean((y_predict - y) ** 2)\n",
    "    return dict(cost=cost, fit_params=best_fit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Instead of inverting matrices, we could just iterate many times to try to find a set of optimal parameters with a sum of squares cost function.  While memory is not a realistic constraint, this method does have the advantage of consuming significantly less memory than matrix inversion at the cost of iteration time.\n",
    "\n",
    "$$l(a, b, c) = \\sum_{i=1}^{N} ((ax_{i}^{2} + bx + c) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    x: np.ndarray, y: np.ndarray, learning_rate: float = 1e-3, max_iterations: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"Given samples of a quadratic function, find optimal parameters with gradient descent using\n",
    "    torch's autograd\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): The inputs to the function we are attempting to approximate\n",
    "        y (np.ndarray): The targets we are going to minimize error against\n",
    "        learning_rate (float, optional): The learning rate to multiply gradients by. Defaults to 1e-3.\n",
    "        max_iterations (int, optional): The maximum number of iterations before terminating the algorithm. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        dict: Optimization outputs such as a gradient, parameter, and cost histories as well as the best parameters found\n",
    "    \"\"\"\n",
    "    # X and Y must be tensors to work with torch\n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.Tensor(y)\n",
    "    # Randomly initialize our a, b, and c guesses to start\n",
    "    parameters = torch.ones(3)\n",
    "    parameters.requires_grad = True\n",
    "\n",
    "    # Track the best set of parameters found so far\n",
    "    best_parameters = None\n",
    "    best_cost = np.inf\n",
    "\n",
    "    # Track iteration count and values of key quantities for each iteration\n",
    "    iteration = 0\n",
    "    costs = []\n",
    "    grads = []\n",
    "    params = []\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        y_predict = parameters[0] * x**2 + parameters[1] * x + parameters[2]\n",
    "        cost = torch.square(y_predict - y).sum()\n",
    "        cost.backward()\n",
    "        print(f\"Iter {iteration} cost: {cost}\")\n",
    "        with torch.no_grad():\n",
    "            gradient = parameters.grad\n",
    "            parameters -= gradient * learning_rate\n",
    "        grads.append(np.array(parameters.grad.detach()))\n",
    "        costs.append(float(cost))\n",
    "        params.append(np.array(parameters.detach()))\n",
    "\n",
    "        if costs[-1] < best_cost:\n",
    "            best_cost = costs[-1]\n",
    "            best_parameters = params[-1]\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return dict(\n",
    "        cost=cost,\n",
    "        costs=costs,\n",
    "        grads=grads,\n",
    "        params=params,\n",
    "        fit_params=best_parameters,\n",
    "        iterations=iteration,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(x, y, targets: np.ndarray, opt_params: np.ndarray):\n",
    "    target_func = quadratic_factory(*targets)\n",
    "    fit_func = quadratic_factory(*opt_params)\n",
    "    print(f\"f`(x) = {opt_params[0]}x^2 + {opt_params[1]}x + {opt_params[2]}\")\n",
    "\n",
    "    # Compute MSE\n",
    "    mse = np.mean((fit_func(x) - y) ** 2)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    plt.plot(x, target_func(x), label=\"F(x)\")\n",
    "    plt.plot(x, y, \"r.\", label=\"Sampled F(x)\")\n",
    "    plt.plot(x, fit_func(x), \"k\", label=\"F`(x)\")\n",
    "    plt.title(f\"f(x) = ${targets[0]}x^2 + {targets[1]}x + {targets[2]}$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 250\n",
    "sigma = 1\n",
    "np.random.seed(30)\n",
    "targets = np.round(np.random.rand(3) - 0.5, 3)\n",
    "x = (np.random.rand(n_samples) - 0.5) * 10\n",
    "x.sort()\n",
    "target_func = quadratic_factory(*targets)\n",
    "sampler = noisy_samples(target_func, sigma=sigma)\n",
    "y = sampler(x)\n",
    "\n",
    "qr_result = quadratic_regression(x, y)\n",
    "plot_result(x, y, targets=targets, opt_params=qr_result[\"fit_params\"])\n",
    "\n",
    "gd_result = gradient_descent(x, y, learning_rate=1e-4)\n",
    "plot_result(x, y, targets=targets, opt_params=gd_result[\"fit_params\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Problems\n",
    "\n",
    "The two differing results above show gradient descent is not a good way to solve this problem (unless gradient descent got very lucky)!  As the number of points gets very large, the quadratic regression solution's loss term will tend toward the variance of the noise - a perfect result considering the noise is uncorrelated.  The gradient descent solution will \"learn\" a better set of parameters than the ones it started with, but will be mostly wrong.  Plotting the gradients and cost may lend some insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gd_result[\"costs\"])\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Cost over Training\")\n",
    "plt.figure()\n",
    "a_grad, b_grad, c_grad = list(zip(*gd_result[\"grads\"]))\n",
    "plt.plot(a_grad, label=\"Grad A\")\n",
    "plt.plot(b_grad, label=\"Grad B\")\n",
    "plt.plot(c_grad, label=\"Grad C\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Gradient Magnitude\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Gradient over Training\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our cost function is oscillating during training, which means our step size is too big for some regions of the cost function.  We know there is a global minimum as we have a closed form, unique solution to our regression problem, so why doesn't gradient descent find it?  Intuitively, we see that there is a steep descent in cost where the step size is appropriate for the cost function\n",
    "\n",
    "Fortunately, we can compute closed form solutions for the first order partial derivatives of our cost function to see if knowing the Jacobian will help:\n",
    "\n",
    "$$\\frac{dl}{da} = 2 \\sum_{i = 1}^{N} x_i^2((ax_i^2 + bx_i + c) - y_i)$$\n",
    "$$\\frac{dl}{db} = 2 \\sum_{i = 1}^{N} x_i((ax_i^2 + bx_i + c) - y_i)$$\n",
    "$$\\frac{dl}{dc} = 2 \\sum_{i = 1}^{N} ax_i^2 + bx_i + c - y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_exact(\n",
    "    x: np.ndarray, y: np.ndarray, learning_rate: float = 1e-3, max_iterations: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"Given samples of a quadratic function, find optimal parameters with gradient descent using a closed form jacobian\"\"\"\n",
    "    # Randomly initialize our a, b, and c guesses to start\n",
    "    a, b, c = np.random.randn(3)\n",
    "\n",
    "    # Track the best set of parameters found so far\n",
    "    best_parameters = None\n",
    "    best_cost = np.inf\n",
    "\n",
    "    # Track iteration count and values of key quantities for each iteration\n",
    "    iteration = 0\n",
    "    costs = []\n",
    "    grads = {\"a\": [], \"b\": [], \"c\": []}\n",
    "    params = []\n",
    "    while iteration < max_iterations:\n",
    "        y_predict = a * x**2 + b * x + c\n",
    "        cost = np.square(y_predict - y).sum()\n",
    "        print(f\"Iter {iteration} cost: {cost}\")\n",
    "\n",
    "        grad_a = 2 * np.mean(x**2 * ((a * x**2 + b * x + c) - y))\n",
    "        grad_b = 2 * np.mean(x * ((a * x**2 + b * x + c) - y))\n",
    "        grad_c = 2 * np.mean(a * x**2 + b * x + c - y)\n",
    "\n",
    "        grads[\"a\"].append(grad_a)\n",
    "        grads[\"b\"].append(grad_b)\n",
    "        grads[\"c\"].append(grad_c)\n",
    "\n",
    "        # Update the parameters\n",
    "        a -= grad_a * learning_rate\n",
    "        b -= grad_b * learning_rate\n",
    "        c -= grad_c * learning_rate\n",
    "\n",
    "        costs.append(float(cost))\n",
    "        params.append(np.array([a, b, c]))\n",
    "\n",
    "        if costs[-1] < best_cost:\n",
    "            best_cost = costs[-1]\n",
    "            best_parameters = params[-1]\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return dict(\n",
    "        cost=cost,\n",
    "        costs=costs,\n",
    "        grads=grads,\n",
    "        params=params,\n",
    "        fit_params=best_parameters,\n",
    "        iterations=iteration,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kgd_result = gradient_descent_exact(x, y, learning_rate=1e-3, max_iterations=150)\n",
    "plot_result(x, y, targets=targets, opt_params=kgd_result[\"fit_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kgd_result[\"costs\"])\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Cost over Training\")\n",
    "plt.figure()\n",
    "a_grad, b_grad, c_grad = [list(x) for x in kgd_result[\"grads\"].values()]\n",
    "plt.plot(a_grad, label=\"Grad A\")\n",
    "plt.plot(b_grad, label=\"Grad B\")\n",
    "plt.plot(c_grad, label=\"Grad C\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Gradient Magnitude\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Gradient over Training\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a closed form solution to the gradient helped with smoothing out our costs and gradients to a point where a steady state was reached after about 100 epochs.  After playing with the learning rate, it looks like .001 is a decent value but it still takes some time for `c` to converge.  There are some lingering instabilities with higher learning rates - try out a learning rate of .1 and gradients should explode and crash the optimization.  Unfortunately, even after all that, we still don't have a very good solution compared with our closed form solution.  We are still lacking the ability to modulate our learning rate without knowledge of the curvature of the cost function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy Optimization\n",
    "\n",
    "While the solution above provides a somewhat satisfying alternative to the problem with original gradient descent, we might as well see what a more carefully designed optimizer can achieve in our problem.  For this, we can look at the `scipy.optimize` package and utilize a host of solvers to try to tackle the problem. \n",
    "\n",
    "We'll leverage BFGS, which requires a function for providing a means to evaluate the jacobian, but performs estimation of the hessian, which will allow the algorithm to adjust the step size in a more intelligent way than pure gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(parameters: np.ndarray) -> float:\n",
    "    \"\"\"Define the cost function for optimization\"\"\"\n",
    "    return np.sqrt(\n",
    "        np.square(\n",
    "            (parameters[0] * x**2 + parameters[1] * x + parameters[2]) - y\n",
    "        ).sum()\n",
    "    )\n",
    "\n",
    "\n",
    "def jacobian(parameters: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Define the jacobian (vector of partial derivatives) for optimization, same as known_gradient_descent\"\"\"\n",
    "    y_prime = parameters[0] * x**2 + parameters[1] * x + parameters[2]\n",
    "    da = 2 * np.sum(x**2 * (y_prime - y))\n",
    "    db = 2 * np.sum(x * (y_prime - y))\n",
    "    dc = 2 * np.sum(y_prime - y)\n",
    "    return np.array([da, db, dc])\n",
    "\n",
    "\n",
    "# Random parameter initialization\n",
    "params = np.random.randn(3)\n",
    "result = scipy.optimize.minimize(fun=objective, x0=params, method=\"BFGS\", jac=jacobian)\n",
    "print(result)\n",
    "plot_result(x, y, targets=targets, opt_params=result.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like BFGS works quite well - less than 10 iterations were required to estimate nearly the exact same set of parameters as our closed for solution.\n",
    "\n",
    "Let's see if we can cut down the amount of time to convergence by providing a hessian, or second derivative, to another optimizer, Newton-CG.  This should converge faster with perfect knowledge of the hessian.  To derive the hessian, we have to differentiate our gradients by each parameter, making a 3x3 matrix.  \n",
    "\n",
    "$$\\frac{dl}{da^2}(a, b, c) = 2 * \\sum_{i = 1}^{N} x^4$$\n",
    "$$\\frac{dl}{dadb}(a, b, c) = 2 * \\sum_{i = 1}^{N} x^3$$\n",
    "$$\\frac{dl}{dadc}(a, b, c) = 2 * \\sum_{i = 1}^{N} x^2$$\n",
    "\n",
    "$$\\frac{dl}{dbda}(a, b, c) = 2 * \\sum_{i = 1}^{N} x^3$$\n",
    "$$\\frac{dl}{db^2}(a, b, c) = 2 * \\sum_{i = 1}^{N} x^2$$\n",
    "$$\\frac{dl}{dbdc}(a, b, c) = 2 * \\sum_{i = 1}^{N} x$$\n",
    "\n",
    "$$\\frac{dl}{dcda}(a, b, c) = 2 * \\sum_{i = 1}^{N} x^4$$\n",
    "$$\\frac{dl}{dcdb}(a, b, c) = 2 * \\sum_{i = 1}^{N} x^3$$\n",
    "$$\\frac{dl}{dc^2}(a, b, c) = 2N $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hessian matrix\n",
    "dada = 2 * np.sum(x**4)\n",
    "dadb = 2 * np.sum(x**3)\n",
    "dadc = 2 * np.sum(x**2)\n",
    "dbda = 2 * np.sum(x**3)\n",
    "dbdb = 2 * np.sum(x**2)\n",
    "dbdc = 2 * np.sum(x)\n",
    "dcda = 2 * np.sum(x**2)\n",
    "dcdb = 2 * np.sum(x)\n",
    "dcdc = 2 * n_samples\n",
    "hessian = np.array([[dada, dadb, dadc], [dbda, dbdb, dbdc], [dcda, dcdb, dcdc]])\n",
    "\n",
    "# Random parameter initialization\n",
    "params = np.random.randn(3)\n",
    "result = scipy.optimize.minimize(\n",
    "    fun=objective,\n",
    "    x0=params,\n",
    "    method=\"Newton-CG\",\n",
    "    jac=jacobian,\n",
    "    # This must be a function, but the hessian matrix is constant, so just return the same thing every time\n",
    "    hess=lambda _: hessian,\n",
    ")\n",
    "print(result)\n",
    "plot_result(x, y, targets=targets, opt_params=result.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, with more information about the cost function's gradient, we get to convergence in fewer iterations.  However, we can do this same thing in a single iteration using Newton's method, as described [https://en.wikipedia.org/wiki/Newton%27s_method#k_variables,_k_functions](here).  All we have to do is initialize a guess and evaluate the jacobian for that guess:\n",
    "\n",
    "$$ \\hat{x} = x_0 - H^{-1} J(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = params - np.dot(np.linalg.inv(hessian), jacobian(params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing that with our regression result, it should be nearly identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(qr_result[\"fit_params\"], solution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "While this was presented as the \"easy intro\" to gradient descent in the fast.ai course, it reveals serious shortcomings of gradient descent when applied to some problems.  There are plenty of other means to solve the quadratic regression (or to generalize, polynomial regression) problem and the method proposed is possibly the worst one available.  While simple to implement and doesn't force the user to differentiate the loss function, this method relies on a very sensitive learning rate parameter to get things right, and the solutions will be far from optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c5e7bc60812e4f465d8a8f03ded68579c8c0c964eec928bebe0bf95b0baae9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
