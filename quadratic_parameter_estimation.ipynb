{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Approaches to Quadratic Parameter Estimation\n",
    "\n",
    "This notebook is an extension of the FastAI course's \"How does a neural net really work?\" notebook that introduces users to `autograd` and `Tensor`.  To best get a feel for the lowest level of PyTorch after working with Lightning for some time, this is a quick trip back to the fundamentals.\n",
    "\n",
    "This notebook is split into the following sections:\n",
    "\n",
    "1. Define a few functions to build arbitrary quadratic functions and samplers that will add some noise to simulate noisy data derived from these functions\n",
    "2. Classic quadratic regression using the Moore-Penrose pseudoinverse to find the solution to the least squares problem.  This will serve as the gold standards for other methods\n",
    "3. Three parameter model using `torch` and attempt to find the quadratic parameters through gradient descent\n",
    "4. The same three parameter model using closed form solutions for the gradient of the loss function\n",
    "5. Using `scipy` to leverage some well designed optimizers to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_factory(a: float, b: float, c: float) -> callable:\n",
    "    \"\"\"Build a function that will return values defined by the quadratic function\n",
    "    parametrized by a, b, and c\n",
    "\n",
    "    Args:\n",
    "        a (float): The squared coefficient\n",
    "        b (float): The linear coefficient\n",
    "        c (float): The constant value\n",
    "\n",
    "    Returns:\n",
    "        callable: A function that returns 'y' values for the quadratic function\n",
    "            parametrized by a, b, and c\n",
    "    \"\"\"\n",
    "\n",
    "    def quadratic(x: float | np.ndarray) -> float | np.ndarray:\n",
    "        \"\"\"The quadratic function\"\"\"\n",
    "        return a * x**2 + b * x + c\n",
    "\n",
    "    return quadratic\n",
    "\n",
    "\n",
    "def noisy_samples(quadratic: callable, sigma: float) -> callable:\n",
    "    \"\"\"Wrap a defined quadratic function with a noise generator\n",
    "    to generate training samples\n",
    "\n",
    "    Args:\n",
    "        quadratic (callable): The defined quadratic function, see above\n",
    "        sigma (float): The variance of the gaussian noise added to every sample\n",
    "\n",
    "    Returns:\n",
    "        callable: A function that returns noisy samples of specified shape\n",
    "    \"\"\"\n",
    "\n",
    "    def noisy_quad(x: float | np.ndarray):\n",
    "        return quadratic(x) + np.random.randn(*x.shape) * sigma\n",
    "\n",
    "    return noisy_quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function and plot it\n",
    "a = 4\n",
    "b = 3\n",
    "c = -1\n",
    "quad_func = quadratic_factory(a, b, c)\n",
    "sampler = noisy_samples(quadratic=quad_func, sigma=5)\n",
    "\n",
    "x = np.linspace(-5, 5, 50)\n",
    "y = quad_func(x)\n",
    "\n",
    "# Generate some samples between [-5, 5]\n",
    "x_sampled = (np.random.rand(50) - 0.5) * 10\n",
    "y_sampled = sampler(x_sampled)\n",
    "\n",
    "plt.plot(x, y, label=\"F(x)\")\n",
    "plt.plot(x_sampled, y_sampled, \"r.\", label=\"Sampled F(x)\")\n",
    "plt.title(f\"f(x) = ${a}x^2 + {b}x + {c}$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Regression\n",
    "\n",
    "The problem we are trying to solve -- finding the optimal coefficients for an unknown quadratic -- can be solved closed-form using simple regression.  However, inverting matrices takes time and more memory than a 3 parameter gradient descent.  Let's draw a baseline with regression and see how it stacks up to gradient descent with a parameter sweep of number of samples and amount of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_regression(x: np.ndarray, y: np.ndarray) -> dict:\n",
    "    \"\"\"Run regression on noisy data to minimize the mean squared error\n",
    "    over three parameters\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): The input x coordinates\n",
    "        y (np.ndarray): The input y coordinates\n",
    "\n",
    "    Returns:\n",
    "        dict: information summarizing the regression\n",
    "    \"\"\"\n",
    "    matrix_x = np.vstack((x**2, x, np.ones_like(x))).T\n",
    "    matrix_x_inv = np.dot(np.linalg.inv(np.dot(matrix_x.T, matrix_x)), matrix_x.T)\n",
    "    best_fit = np.dot(matrix_x_inv, y)\n",
    "    y_predict = best_fit[0] * x**2 + best_fit[1] * x + best_fit[2]\n",
    "    cost = np.mean((y_predict - y) ** 2)\n",
    "    return dict(cost=cost, fit_params=best_fit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Instead of inverting matrices, we could just iterate many times to try to find a set of optimal parameters with the same mean squared error cost function.  While memory is not a realistic constraint, this method does have the advantage of consuming significantly less memory than matrix inversion at the cost of iteration time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    x: np.ndarray, y: np.ndarray, learning_rate: float = 1e-3, max_iterations: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"Given samples of a quadratic function, find optimal parameters with gradient descent using \n",
    "    torch's autograd\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): The inputs to the function we are attempting to approximate\n",
    "        y (np.ndarray): The targets we are going to minimize error against\n",
    "        learning_rate (float, optional): The learning rate to multiply gradients by. Defaults to 1e-3.\n",
    "        max_iterations (int, optional): The maximum number of iterations before terminating the algorithm. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        dict: Optimization outputs such as a gradient, parameter, and cost histories as well as the best parameters found\n",
    "    \"\"\"\n",
    "    # X and Y must be tensors to work with torch\n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.Tensor(y)\n",
    "    # Randomly initialize our a, b, and c guesses to start\n",
    "    parameters = torch.ones(3)\n",
    "    parameters.requires_grad = True\n",
    "\n",
    "    # Track the best set of parameters found so far\n",
    "    best_parameters = None\n",
    "    best_cost = np.inf\n",
    "    \n",
    "    # Track iteration count and values of key quantities for each iteration\n",
    "    iteration = 0\n",
    "    costs = []\n",
    "    grads = []\n",
    "    params = []\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        y_predict = parameters[0] * x**2 + parameters[1] * x + parameters[2]\n",
    "        cost = torch.square(y_predict - y).mean()\n",
    "        cost.backward()\n",
    "        print(f\"Iter {iteration} cost: {cost}\")\n",
    "        with torch.no_grad():\n",
    "            gradient = parameters.grad\n",
    "            parameters -= gradient * learning_rate\n",
    "        grads.append(np.array(parameters.grad.detach()))\n",
    "        costs.append(float(cost))\n",
    "        params.append(np.array(parameters.detach()))\n",
    "\n",
    "        if costs[-1] < best_cost:\n",
    "            best_cost = costs[-1]\n",
    "            best_parameters = params[-1]\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return dict(\n",
    "        cost=cost,\n",
    "        costs=costs,\n",
    "        grads=grads,\n",
    "        params=params,\n",
    "        fit_params=best_parameters,\n",
    "        iterations=iteration,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(x, y, targets: np.ndarray, opt_params: np.ndarray):\n",
    "    target_func = quadratic_factory(*targets)\n",
    "    fit_func = quadratic_factory(*opt_params)\n",
    "    print(f\"f`(x) = {opt_params[0]}x^2 + {opt_params[1]}x + {opt_params[2]}\")\n",
    "\n",
    "    # Compute MSE\n",
    "    mse = np.mean((fit_func(x) - y) ** 2)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    plt.plot(x, target_func(x), label=\"F(x)\")\n",
    "    plt.plot(x, y, \"r.\", label=\"Sampled F(x)\")\n",
    "    plt.plot(x, fit_func(x), \"k\", label=\"F`(x)\")\n",
    "    plt.title(f\"f(x) = ${targets[0]}x^2 + {targets[1]}x + {targets[2]}$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 250\n",
    "sigma = 1\n",
    "np.random.seed(30)\n",
    "targets = np.round(np.random.rand(3) - .5, 3)\n",
    "x = (np.random.rand(n_samples) - 0.5) * 10\n",
    "x.sort()\n",
    "target_func = quadratic_factory(*targets)\n",
    "sampler = noisy_samples(target_func, sigma=sigma)\n",
    "y = sampler(x)\n",
    "\n",
    "qr_result = quadratic_regression(x, y)\n",
    "plot_result(x, y, targets=targets, opt_params=qr_result['fit_params'])\n",
    "\n",
    "gd_result = gradient_descent(x, y, learning_rate=1e-4)\n",
    "plot_result(x, y, targets=targets, opt_params=gd_result['fit_params'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Problems\n",
    "\n",
    "The two differing results above show gradient descent is not a good way to solve this problem (unless gradient descent got very lucky)!  As the number of points gets very large, the quadratic regression solution's loss term will tend toward the variance of the noise - a perfect result considering the noise is uncorrelated.  The gradient descent solution will \"learn\" a better set of parameters than the ones it started with, but will be mostly wrong.  Plotting the gradients and cost may lend some insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gd_result['costs'])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Cost over Training')\n",
    "plt.figure()\n",
    "a_grad, b_grad, c_grad = list(zip(*gd_result['grads']))\n",
    "plt.plot(a_grad, label='Grad A')\n",
    "plt.plot(b_grad, label='Grad B')\n",
    "plt.plot(c_grad, label='Grad C')\n",
    "plt.legend()\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Gradient over Training')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our cost function is oscillating during training, which means our step size is too big and is overshooting the global minimum.  We know there is a global minimum as we have a closed form, unique solution to our regression problem, so why doesn't gradient descent find it?  It looks like our gradients have huge differences in magnitude, which may have something to do with it.  Perhaps small changes in `a` dominate the cost function and the correct value of `a` is located at the bottom of a very narrow valley which would require a very slow learning rate to traverse.\n",
    "\n",
    "Fortunately, we can compute closed form solutions for the first order partial derivatives of our cost function to see if approximations are the problem:\n",
    "\n",
    "$$l(a, b, c) = \\sum_{i=1}^{N} ((ax_{i}^{2} + bx + c) - y_i^2)$$\n",
    "\n",
    "\n",
    "$$\\frac{dl}{da} = \\frac{2}{n} \\sum_{i = 1}^{N} x_i^2((ax_i^2 + bx_i + c) - y_i)$$\n",
    "$$\\frac{dl}{db} = \\frac{2}{n} \\sum_{i = 1}^{N} x_i((ax_i^2 + bx_i + c) - y_i)$$\n",
    "$$\\frac{dl}{dc} = \\frac{2}{n} \\sum_{i = 1}^{N} ax_i^2 + bx_i + c - y_i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_gradient_descent(\n",
    "    x: np.ndarray, y: np.ndarray, learning_rate: float = 1e-3, max_iterations: int = 100\n",
    ") -> dict:\n",
    "    \"\"\"Given samples of a quadratic function, find optimal parameters with gradient descent using precomputed gradient functions\"\"\"\n",
    "    # Randomly initialize our a, b, and c guesses to start\n",
    "    a, b, c = np.random.randn(3)\n",
    "\n",
    "    # Track the best set of parameters found so far\n",
    "    best_parameters = None\n",
    "    best_cost = np.inf\n",
    "    \n",
    "    # Track iteration count and values of key quantities for each iteration\n",
    "    iteration = 0\n",
    "    costs = []\n",
    "    grads = {'a':[], 'b': [], 'c': []}\n",
    "    params = []\n",
    "    while iteration < max_iterations:\n",
    "        y_predict = a * x**2 + b * x + c\n",
    "        cost = np.square(y_predict - y).mean()\n",
    "        print(f\"Iter {iteration} cost: {cost}\")\n",
    "\n",
    "        grad_a = 2 * np.mean(x**2 * ((a * x ** 2 + b * x + c) - y))\n",
    "        grad_b = 2 * np.mean(x* ((a * x ** 2 + b * x + c) - y))\n",
    "        grad_c = 2 * np.mean(a * x ** 2 + b * x + c - y)\n",
    "\n",
    "        grads['a'].append(grad_a)\n",
    "        grads['b'].append(grad_b)\n",
    "        grads['c'].append(grad_c)\n",
    "\n",
    "        # Update the parameters\n",
    "        a -= grad_a * learning_rate\n",
    "        b -= grad_b * learning_rate\n",
    "        c -= grad_c * learning_rate\n",
    "\n",
    "        costs.append(float(cost))\n",
    "        params.append(np.array([a, b, c]))\n",
    "\n",
    "        if costs[-1] < best_cost:\n",
    "            best_cost = costs[-1]\n",
    "            best_parameters = params[-1]\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return dict(\n",
    "        cost=cost,\n",
    "        costs=costs,\n",
    "        grads=grads,\n",
    "        params=params,\n",
    "        fit_params=best_parameters,\n",
    "        iterations=iteration,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kgd_result = known_gradient_descent(x, y, learning_rate=1e-3, max_iterations=150)\n",
    "plot_result(x, y, targets=targets, opt_params=kgd_result['fit_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kgd_result['costs'])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Cost over Training')\n",
    "plt.figure()\n",
    "a_grad, b_grad, c_grad = [list(x) for x in kgd_result['grads'].values()]\n",
    "plt.plot(a_grad, label='Grad A')\n",
    "plt.plot(b_grad, label='Grad B')\n",
    "plt.plot(c_grad, label='Grad C')\n",
    "plt.legend()\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Gradient over Training')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a closed form solution to the gradient helped with smoothing out our costs and gradients to a point where a steady state was reached after about 100 epochs.  After playing with the learning rate, it looks like .001 is a decent value but it still takes some time for `c` to converge.  There are some lingering instabilities with higher learning rates - try out a learning rate of .1 and gradients should explode and crash the optimization.  Unforunately, even after all that, we still don't have a very good solution compared with our closed form solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy Optimization\n",
    "\n",
    "While the solution above provides a somewhat satisfying alternative to the problem with original gradient descent, we might as well see what a more carefully designed optimizer can achieve in our problem.  For this, we can look at the `scipy.optimize` package and utilize a host of solvers to try to tackle the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(parameters: np.ndarray) -> float:\n",
    "    \"\"\"Define the cost function for optimization\"\"\"\n",
    "    return np.square((parameters[0] * x ** 2 + parameters[1] * x + parameters[2]) - y).mean()\n",
    "\n",
    "def jacobian(parameters: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Define the jacobian (vector of partial derivatives) for optimization, same as known_gradient_descent\"\"\"\n",
    "    y_prime = parameters[0] * x ** 2 + parameters[1] * x + parameters[2]\n",
    "    da = 2 * np.mean(x ** 2 * (y_prime - y))\n",
    "    db = 2 * np.mean(x * (y_prime - y))\n",
    "    dc = 2 * np.mean(y_prime - y)\n",
    "    return np.array([da, db, dc])\n",
    "\n",
    "# Random parameter initialization\n",
    "params = np.random.randn(3)\n",
    "result = scipy.optimize.minimize(fun=objective, x0=params, method='BFGS', jac=jacobian)\n",
    "print(result)\n",
    "plot_result(x, y, targets=targets, opt_params=result.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is virtually identical to our closed form matrix solution and it converges in less than 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "While this was presented as the \"easy intro\" to gradient descent in the fast.ai course, it reveals serious shortcomings of gradient descent when applied to some problems.  There are plenty of other means to solve the quadratic regression (or to generalize, polynomial regression) problem and the method proposed in the course is possibly the worst one an engineer could choose.  While simple to implement and lightweight on memory, this method relies on a very sensitive learning rate parameter to get things right, and the solutions will be far from optimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c5e7bc60812e4f465d8a8f03ded68579c8c0c964eec928bebe0bf95b0baae9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
